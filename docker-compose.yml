version: '3.8'

x-airflow-common: &airflow-common
  image: custom-airflow-ecom:latest
  build: .
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@postgres/airflow_db
    - AIRFLOW_UID=${AIRFLOW_UID:-50000}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./spark_scripts:/opt/airflow/spark_scripts
  networks:
    - data_eng_net
  depends_on:
    - postgres

services:
  postgres:
    image: postgres:13
    container_name: pet-project-postgres
    hostname: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=ecom_db
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    networks:
      - data_eng_net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - data_eng_net

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    hostname: kafka
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - data_eng_net

  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports: ["9001:9001", "9000:9000"]
    command: server /data --console-address ":9001"
    volumes: [minio_data:/data]
    networks:
      - data_eng_net

  mc:
    image: minio/mc
    depends_on: [minio]
    container_name: mc
    networks: [data_eng_net]
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set local http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb local/company-datalake;
      /usr/bin/mc mb local/company-datalake/output;
      /usr/bin/mc policy set public local/company-datalake;
      exit 0;
      "

  clickhouse-server:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-server
    hostname: clickhouse-server
    ports:
      - "8123:8123" # HTTP-интерфейс
      - "9002:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse/
    networks:
      - data_eng_net
    ulimits:
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144

  clickhouse-client:
    image: clickhouse/clickhouse-client:latest
    container_name: clickhouse-client
    command: ['--host', 'clickhouse-server']
    depends_on:
      - clickhouse-server
    networks:
      - data_eng_net

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    hostname: spark-master
    ports: ["8080:8080", "7077:7077"]
    volumes: ["./jars:/opt/bitnami/spark/custom-jars"]
    networks:
      - data_eng_net

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes: ["./jars:/opt/bitnami/spark/custom-jars"]
    networks:
      - data_eng_net

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports: ["8081:8080"]
    depends_on:
      airflow-scheduler:
        condition: service_healthy

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--local"]
      interval: 10s
      timeout: 10s
      retries: 5
  
  jupyter-lab:
    image: jupyter/pyspark-notebook:spark-3.5.0
    container_name: jupyter-lab
    ports: ["8888:8888"]
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_OPTS=--driver-java-options=-Dlog4j.logLevel=ERROR --conf spark.driver.extraClassPath=/opt/bitnami/spark/custom-jars/* --conf spark.executor.extraClassPath=/opt/bitnami/spark/custom-jars/* --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=minioadmin --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    volumes: ["./jars:/opt/bitnami/spark/custom-jars", "./notebooks:/home/jovyan/work"]
    networks:
      - data_eng_net

volumes:
  pg_data:
  minio_data:
  clickhouse_data:

networks:
  data_eng_net:
    driver: bridge